{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GA0Gt3cuHLo"
   },
   "source": [
    "# **DATA 2060** – *Final Project*\n",
    "### **Machine Learning - from theory to algorithms**\n",
    "\n",
    "**Edouard Clocheret, Junhui Huang, Eric Fan, Shivam Hingorani**\n",
    "\n",
    "**Link to the github repository:** https://github.com/edouardclocheret/GaussianNaiveBayes.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXe07RUbvNuy"
   },
   "source": [
    "## 1. Overview of Gaussian Naive Bayes for classification\n",
    "\n",
    "Gaussian Naive Bayes is an extension of the conventional Naive Bayes algorithm. Like the original, the Gaussian variety of Naive Bayes is used for classification tasks, specifically those where the features are continuous and can be assumed to follow a Gaussian (i.e., normal) distribution. This means Gaussian Naive Bayes maintains the assumption of conditional independence across features (hence the description as \"naive\"). This algorithm is computationally efficient, which can be advantageous when using large datasets,and it serves as the basis for many more complex machine learning models/techniques.\n",
    "\n",
    "### REPRESENTATION\n",
    "\n",
    "#### Multiclass Naive Bayes Representation:\n",
    "\n",
    "For a given input: $X = (x_1,x_2,...,x_n)$,\n",
    "\n",
    " ##### **First**, we calculate the prior probability for each class,\n",
    " which is defined as the probability of each class based on the training data. This is donated as\n",
    "$$ P(C_k) = \\frac{\\text{Number of samples in class } C_k}{\\text{Total number of samples}} = \\frac{N_k}{N}$$\n",
    "\n",
    "\n",
    "##### **Second**, For each class, we calculate the likelihood $P(X | C_k) $,\n",
    "which is the proabability of observing feature $X$ given that the data point belongs to a specific Class $C_k$\n",
    "\n",
    "To calculate this, we use the Naive Bayes assumption that each feature $x_i$ is independent of others given the class C. This allows us to calculate the product of the individual feature likelihoods:\n",
    "\n",
    "$$P(X | C_k) = \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "##### **Third**, We calculate the posterior Probability $P(C_k | X)$ for each class,\n",
    "which is the goal of this algorithim, calculating the probability of each class C based on the feature X. Since\n",
    "$P(X)$ is the same for all classes, we can ignore it when comparing classes, so we don’t need to calculate it explicitly.\n",
    "$$P(C_k | X) \\propto P(C_k) \\cdot \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "where\n",
    "$$\n",
    "P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp \\left(-\\frac{(x_i - \\mu_k)^2}{2\\sigma_k^2}\\right)\n",
    "$$\n",
    "\n",
    "##### **Last**, We select the class C with the highest posterior probability,\n",
    "which means that we After computing $P(C_k∣X)$ for each class $C_k$ , choose the class with the highest posterior probability as the predicted class:\n",
    "\n",
    "$${C} = \\arg \\max_{C_k} P(C_k∣X) = \\arg \\max_{C_k} P(C_k) \\cdot \\prod_{i=1}^{n} P(x_i | C_k)$$\n",
    "\n",
    "### LOSS\n",
    "\n",
    "For Gaussian Naive Bayes (GNB), the loss function normally not defined or refined. Because this type of model deduct based on **Maximum Likelihood Estimation** (MLE).\n",
    "\n",
    "However, if we have to define a loss function when during the Training or Evaluation Process, we can define *Negative Log Likelihood* (NLL) to measure the classification loss.\n",
    "\n",
    "The formula:\n",
    "\n",
    "$$\n",
    "\\text{NLL} = -\\sum_{i=1}^{N} \\log P(y_i | X_i)\n",
    "$$\n",
    "\n",
    "$y_i$ is the $i$ th sample's true label, $X_i$ is input sample feature, $P(y_i|X_i)$ is the probability of model predict.\n",
    "\n",
    "Since Gaussian Naive Bayes assumes that features follow a Gaussian (normal) distribution, it calculates the conditional probability using the probability density function of a Gaussian distribution. For each class $ y $ and feature $ x_i $, the conditional probability \\( P(x_i | y) \\) is given by:\n",
    "\n",
    "$$\n",
    "P(x_i | y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^2}} \\exp \\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)\n",
    "$$\n",
    "Such that the negative log likelihood for Gaussian Naive Bayes is:\n",
    "\n",
    "$$\n",
    "\\text{NLL}_{\\text{Gaussian}} = -\\sum_{i=1}^{N} \\left( \\log P(y_i) + \\sum_{j=1}^{d} \\log \\frac{1}{\\sqrt{2\\pi \\sigma_{y_i,j}^2}} - \\frac{(X_{ij} - \\mu_{y_i,j})^2}{2\\sigma_{y_i,j}^2} \\right)\n",
    "$$\n",
    "$\\mu_{y_i,j} $ and  $ \\sigma_{y_i,j} $ are sample $j$ 's mean and variance under label $y_i$\n",
    "\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "Suppose we have a set of data points $ X = \\{x_1, x_2, \\dots, x_n\\} $, and these data points are drawn from a normal distribution with mean $ \\mu $ and variance $ \\sigma^2 $. We want to know $ \\mu $ and $ \\sigma^2 $ by using MLE.\n",
    "\n",
    "The PDF of the normal distribution is:\n",
    "\n",
    "$$\n",
    "P(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "For a given dataset $ X = \\{x_1, x_2, \\dots, x_n\\} $, the joint likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} P(x_i|\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Taking the log of the likelihood function, we get the log-likelihood function:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2) = -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $ \\mu $ and $ \\sigma^2 $ and setting them to zero, we obtain:\n",
    "\n",
    "- The MLE for the mean:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "  $$\n",
    "\n",
    "- The MLE for the variance:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "### OPTIMIZER\n",
    "\n",
    "The Gaussian assumption of the model is to consider the probabilities $P(x_i | C_k)$ to follow a Gaussian distribution:\n",
    "\n",
    "$$P(x_i | C_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp \\left( -\\frac{(x_i - \\mu_C)^2}{2 \\sigma_C^2} \\right)$$\n",
    "\n",
    "The mean $\\mu$ and variance $\\sigma$ estimators are derived from maximum likelihood estimation (MLE).\n",
    "\n",
    "We use the log-loss because it is easier to calculate derivatives of a sum:\n",
    "\n",
    "$$\\text{LogLoss}(C, X) = - \\log P(C) - \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma_{C,i}^2}} \\exp \\left( -\\frac{(x_i - \\mu_{C,i})^2}{2 \\sigma_{C,i}^2} \\right) \\right)$$\n",
    "\n",
    "and simplifies as : $$\\text{LogLoss}(C, X) = - \\log P(C) + \\sum_{i=1}^n \\left( \\frac{(x_i - \\mu_{C,i})^2}{2 \\sigma_{C,i}^2} + \\log \\sqrt{2 \\pi \\sigma_{C,i}^2} \\right)$$\n",
    "\n",
    "We maximize the likelihood (= minimize the loss) by taking the logarithm (log-likelihood) and setting the derivatives with respect to $\\mu_{C,i}$ and $\\sigma_{C,i}$ to zero, we obtain:\n",
    "$$\\mu_{C_k, i} = \\frac{1}{N_C} \\sum_{j=1}^{N_C} x_{i}^{(j)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\sigma_{C_k, i}^2 = \\frac{1}{N_C} \\sum_{j=1}^{N_C} \\left( x_{i}^{(j)} - \\mu_{C_k, i} \\right)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "* $N_C$  is the number of samples in class C.\n",
    "* $x_i^{(j)}$ is the value of feature $x_i$ for the $j$-th sample in class C.\n",
    "---\n",
    "\n",
    "**In practice**, we first separate the data by class and use the optimal estimators above for the mean and variance.\n",
    "\n",
    "The training consists of computing and memorizing $\\mu_{C_k, i}$ and $\\sigma_{C_k, i}^2$ for all feature $x_i$ and classes $C_k$.\n",
    "\n",
    "These values are then used for the classifications to compute $P(x_i | C_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8pNVYmtwPz5"
   },
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wtwlk_Jhwf3I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian(x, mu, sigma2):\n",
    "\n",
    "    sigma2 = np.where(sigma2 == 0, 1e-6, sigma2)    # Change by GF\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * sigma2)\n",
    "    exponent = np.exp(-((x - mu) ** 2) / (2 * sigma2))\n",
    "\n",
    "    return coeff * exponent\n",
    "\n",
    "class GaussNaiveBayes:\n",
    "    \"\"\" Gaussian Naive Bayes model for multiclass classification\n",
    "\n",
    "    @attrs:\n",
    "        n_classes:    the number of classes\n",
    "        feature_dist:    a 3D (n_classes x n_features x 2) NumPy array of\n",
    "                         the attribute distributions: mean and sigma2 for\n",
    "                         each feature for each class\n",
    "\n",
    "                        ex : [[[mu, sigma2],   of feature 1\n",
    "                               [mu, sigma2],   of feature 2\n",
    "                               [mu, sigma2]],  of feature 3 of class 1\n",
    "\n",
    "                               [[mu, sigma2],  of feature 1\n",
    "                               [mu, sigma2],   of feature 2\n",
    "                               [mu, sigma2]]]  of feature 3 of class 2\n",
    "\n",
    "        priors: a 1D NumPy array of the priors distribution\n",
    "\n",
    "                        ex : [p_class1, p_class2, p_class3]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.n_classes = None   # Computed at training\n",
    "        self.feature_dist = None\n",
    "        self.priors = None\n",
    "\n",
    "        self.laplace_smoothing = 1  # Default value for the smoothing parameter\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\" Trains the model, using maximum likelihood estimation.\n",
    "        @params:\n",
    "            X_train: a 2D (n_examples x n_features) numpy array\n",
    "            y_train: a 1D (n_examples) numpy array of the corresponding labels\n",
    "        @return:\n",
    "            self.priors\n",
    "            self.feature_dist\n",
    "        \"\"\" # Change by GF\n",
    "        \n",
    "        self.n_classes = len(set(y_train))\n",
    "        n_features = X_train.shape[1]\n",
    "\n",
    "        self.priors = np.zeros(self.n_classes)\n",
    "        self.feature_dist = np.zeros((self.n_classes, n_features, 2))\n",
    "\n",
    "        n_examples = X_train.shape[0]\n",
    "\n",
    "        for class_label in range(self.n_classes):\n",
    "            X_class = X_train [y_train == class_label]\n",
    "\n",
    "            # Computing prior with Laplace smoothing\n",
    "            total_class = X_class.shape[0]\n",
    "            a = self.laplace_smoothing\n",
    "            self.priors[class_label] = (total_class +a) /(n_examples +a * self.n_classes)\n",
    "\n",
    "            # Computing the moments (mean mu and variance sigma2)\n",
    "            mu = np.mean(X_class, axis=0)\n",
    "            sigma2 = np.var(X_class, axis = 0) + 1e-6\n",
    "\n",
    "            # Saving the mu and sigma for later predictions\n",
    "            self.feature_dist [class_label, :, :] = np.vstack((mu , sigma2)).T  # Change by GF\n",
    "        \n",
    "        return self.priors, self.feature_dist\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Outputs a predicted label for each input in inputs.\n",
    "\n",
    "        @params:\n",
    "            inputs: a 2D NumPy array containing inputs (n_requests * n_features)\n",
    "        @return:\n",
    "            a 1D numpy array of predictions\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        \n",
    "        for index, input in enumerate(inputs) :\n",
    "\n",
    "            logprobs = np.log(self.priors)\n",
    "\n",
    "            for class_label in range(self.n_classes):\n",
    "                mu = self.feature_dist[class_label, :, 0] # All mus for each feature at a time\n",
    "                sigma2 = self.feature_dist[class_label, :, 1]\n",
    "\n",
    "                logprobs[class_label] += np.sum(np.log(gaussian(input, mu, sigma2) + 1e-6))\n",
    "\n",
    "            predictions[index] = np.argmax(logprobs)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def accuracy(self, X_test, y_test):\n",
    "        \"\"\" Outputs the accuracy of the trained model on a given dataset (data).\n",
    "\n",
    "        @params:\n",
    "            X_test: a 2D numpy array of examples\n",
    "            y_test: a 1D numpy array of labels\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V4cFxUMwiS6"
   },
   "source": [
    "## 3. Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rm3djbLewySZ",
    "outputId": "5feaeaa5-1dec-4088-8660-7ccaefdbfac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test complete\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "test_model1 = GaussNaiveBayes()\n",
    "\n",
    "x1 = np.array([[0,0,1], [0,1,0], [1,0,1], [1,1,1], [0,0,1]])\n",
    "y1 = np.array([0,0,1,1,0])\n",
    "x_test1 = np.array([[1,0,0],[0,0,0],[1,1,1],[0,1,0], [1,1,0]])\n",
    "y_test1 = np.array([0,0,1,0,1])\n",
    "\n",
    "x2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,1,1], [0,0,0], [1,1,0]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_test2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,0,0]])\n",
    "y_test2 = np.array([0,1,1,0])\n",
    "\n",
    "pri1, feature1 = test_model1.train(x1,y1)\n",
    "assert (np.array(pri1) == pytest.approx(np.array([0.571,0.428]),0.01))  # Check if the train function return prior have correct value and shape\n",
    "feature1 = feature1.flatten()\n",
    "cmp = np.array([[[0.00, 0.00], [0.333, 0.222], [0.667, 0.222]], [[1.00, 0.00], [0.50, 0.25], [1.00, 0.00]]]).flatten()\n",
    "assert (feature1 == pytest.approx(cmp, abs = 0.01)) # Check if the train function return feature have correct value and shape\n",
    "\n",
    "assert test_model1.accuracy(x1,y1) == 1.    # Check the accuracy function\n",
    "assert test_model1.accuracy(x2,y2) >= .8\n",
    "print(\"test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im5KaTK6DyqT"
   },
   "source": [
    "### Recreating previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6nqjwC4xD3Mz"
   },
   "outputs": [],
   "source": [
    "#Load and preprocess data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor).\n",
    "Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "'''\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mRoMePqD580",
    "outputId": "ff4e570f-7f50-4390-bb27-f52757fe7f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train accuracy:\n",
      "0.95\n",
      "------------------------------------------------------------\n",
      "Test accuracy:\n",
      "1.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train-test on real data\n",
    "model1 = GaussNaiveBayes()\n",
    "model1.train(X_train, y_train)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "print(\"Train accuracy:\")\n",
    "m1train = model1.accuracy(X_train, y_train)\n",
    "print(m1train)  # Change by GF\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "print(\"Test accuracy:\")\n",
    "m1test = model1.accuracy(X_test, y_test)\n",
    "print(m1test)   # Change by GF\n",
    "\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plvUNMP5EB2l",
    "outputId": "68f3f97f-cfa7-45e3-a683-14e9f254a962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mission Complete\n"
     ]
    }
   ],
   "source": [
    "# Comparison with scikit-learn\n",
    "'''\n",
    "Use the sklearn model to test whether our model is correct or not.\n",
    "'''\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "# If conditional statement executes, our model had the exact same accuracy as the sklearn model\n",
    "if train_accuracy == m1train and test_accuracy == m1test:\n",
    "    print(\"Mission Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebUfxX-6qYz0"
   },
   "source": [
    "#### Comparing results\n",
    "\n",
    "In the previous work we found, which was a journal paper from International Journal of Computer Science and Mobile Computing, they apply the Gaussian Naive Bayes algorithm to classify the Iris dataset into its three species: Iris setosa, Iris versicolor, and Iris virginica.\n",
    "\n",
    "The dataset consists of 150 samples, with 50 samples per species, and includes four features for classification: sepal length (cm), sepal width (cm), petal length (cm), and petal width (cm). The goal is to utilize these features to predict the class of a flower. They also used scatter plot matrix as part of their EDA to study the relationships between pairs of features.\n",
    "\n",
    "In their results, they evaluated the performance of the Gaussian Naive Bayes classifier using metrics such as precision (0.95), recall (0.95), F1-score (0.95), and accuracy (0.95).  Additionally, they used confusion matrix to explain the performance of the Gaussian Naive Bayes classifier.\n",
    "\n",
    "The results from our algorithm closely match their findings and are also consistent with the outcomes produced by the built-in scikit-learn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btOU_zD9w1zQ"
   },
   "source": [
    "## 4: Github repo\n",
    "\n",
    "**Link to the github repository:** https://github.com/edouardclocheret/GaussianNaiveBayes.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaV_l2edxaco"
   },
   "source": [
    "## 5. References\n",
    "\n",
    "1. Wikipedia, 2023. Naive Bayes Classifier. [online] Available at: https://en.wikipedia.org/wiki/Naive_Bayes_classifier [Accessed 8 Dec. 2024].\n",
    "\n",
    "2. GeeksforGeeks, 2023. Gaussian Naive Bayes. [online] Available at: https://www.geeksforgeeks.org/gaussian-naive-bayes/ [Accessed 8 Dec. 2024].\n",
    "\n",
    "3. Brown University, n.d. DATA2060, Assignment 6. [online] Brown University. Available at: [Accessed 8 Dec. 2024].\n",
    "\n",
    "4. Scikit-learn, 2024. Naive Bayes. [online] Available at: https://scikit-learn.org/1.5/modules/naive_bayes.html [Accessed 8 Dec. 2024].\n",
    "\n",
    "5. Iqbal, Z. and Yadav, M., 2020. Multiclass Classification with Iris Dataset Using Gaussian Naive Bayes. International Journal of Computer Science and Mobile Computing, 9(4), pp.27–35. Available at: [Link](https://d1wqtxts1xzle7.cloudfront.net/63087977/V9I420201520200425-125134-1ntewhy-libre.pdf?1587829843=&response-content-disposition=inline%3B+filename%3DMulticlass_Classification_with_Iris_Data.pdf&Expires=1733780084&Signature=eXXIKbxnWYPXgI30onLxTxa2ExMkVgR5y6x2P~jqT6g~cOaH6dt2seXgqALUmNtgtMyjb0gtxiko2kK5DJn0yPM6b3j2Yh0vx2RFOtCN8Y44ZGGGi3CLDNYqYTXPMk24b4zudVToOqVGQud5hLcJz~-NHQLIig5hXHCEH5iTJpVjg-Z34CL4TcElskgvCkdU3U9EDa-SDobkp55glXCUqe5gHFEevVOzaFIOZpPy08G2uW8fSdm69nKHiZal9jHtUJuo5KrPPC8FrHrlBLOTFlsapEs6VtfeRSz2FGamxZMnhjYGvOOMMGVLWk6rSo~2XdAqeD2r6x0cwUUf2WU3Yw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA) [Accessed 8 Dec. 2024].\n",
    "\n",
    "6. UCI Machine Learning Repository, n.d. Iris Data Set. [online] Available at: https://archive.ics.uci.edu/ml/datasets/Iris [Accessed 8 Dec. 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1osTqL4wq-A"
   },
   "source": [
    "## 6. Contributions\n",
    "\n",
    "**Edouard:** Coded the core of the model (```train()```, ```predict()```, ```accuracy()```) and worked on the theory and likelyhood maximization.\n",
    "\n",
    "**Eric:** Developed unit tests and worked on the theory (loss and optimizer).\n",
    "\n",
    "**Huang:** Found previous work where Naive Bayes is applied on Iris dataset. Discusssion of previous work, and reproduction of the previous work. Introducted the representation of the Gaussian Naive Bayes.\n",
    "\n",
    "**Shivam:** Developed replication of previous work and analyzed results. Researched limitations of Gaussian Naive Bayes Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai1PzawSxhZM"
   },
   "source": [
    "## APPENDIX: Student Performance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZIkeKmW8nf0",
    "outputId": "ea02d442-23da-4007-d2c0-49d1c731ef0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "------------------------------------------------------------\n",
      "Train Accuracy: 0.7955390334572491\n",
      "------------------------------------------------------------\n",
      "Test Accuracy: 0.8\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Import kaggle dataset and show training and testing error.\n",
    "file_path = '/content/drive/My Drive/Student_performance_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "y = df['GradeClass']\n",
    "X = df.drop(columns=['GradeClass', 'StudentID'])\n",
    "\n",
    "minmax_ftrs = ['Age', 'StudyTimeWeekly', 'Absences']\n",
    "std_ftrs = ['GPA']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GaussianNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VueVWPEltWm",
    "outputId": "24658fe1-f0d8-4b88-d1bc-5a0bc49ad649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "------------------------------------------------------------\n",
      "Test Accuracy: 0.7978624535315985\n",
      "------------------------------------------------------------\n",
      "Test Accuracy: 0.7708333333333334\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_path = '/content/drive/My Drive/Student_performance_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "y = df['GradeClass']\n",
    "X = df.drop(columns=['GradeClass', 'StudentID'])\n",
    "\n",
    "# Standardize numerical features\n",
    "numeric_features = ['Age', 'StudyTimeWeekly', 'Absences', 'GPA']\n",
    "scaler = StandardScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Initialize and train the GaussNaiveBayes model\n",
    "model1 = GaussNaiveBayes()\n",
    "model1.train(X_train_np, y_train_np)\n",
    "\n",
    "# Calculate train and test accuracies\n",
    "print(\"------------------------------------------------------------\")\n",
    "m1train = model1.accuracy(X_train_np, y_train_np)\n",
    "print(f\"Test Accuracy: {m1train}\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Test Accuracy: {m1test}\")\n",
    "print(\"------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
